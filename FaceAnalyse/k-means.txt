/************************
		k-means算法
*************************/
算法描述：
K-means算法以距离作为指标，度量样本间存在的关系。

算法步骤：
1、初始化质心，初始化规则随机抽取，即从N个样本数据中随机抽取K个样本作为初始质心；
2、遍历样本容器，计算各个样本到质心的距离，计算规则为欧式距离；
3、对样本进行归类，归类规则为最邻近原则，即，样本与该样本距离最近的质心为一类
4、计算新生成的各簇质心，计算规则为取簇内样本均值点为该簇的质心；
5、比照原质心，若新旧质心未发生变化，则算法收敛，输出计算结果。否则转步骤2.


算法设计：
	算法输入：样本数据、簇数
	算法输出：各样本所属的簇
	样本设计：样本属性包含样本数据、样本维数以及样本所属簇。
	过程设计：包含质心初始化、距离计算和样本归类、新质心计算和新旧质心比照、条件循环
	过程变量设计：存储质心数组变量、存储各个样本与质心的距离数组变量。

样本数据结构：
typedef struct Sample{
int cluster;
vector<float> data;
};


算法优缺点：
优点：简单、快速
缺点：
1、k需要预先给定；
2、初始聚类中心随机选择对聚类结果影响较大(即于SSE是一个非凸函数（non-convex function），
所以SSE不能保证找到全局最优解，只能确保局部最优解。但是可以重复执行几次kmeans，选取SSE最小的一次作为最终的聚类结果。)；
3、使用欧式距离，只适合不相关样本数据。

时间复杂度：O(I*n*k*m)
空间复杂度：O(n*m)
其中m为每个元素字段个数，n为数据量，I为跌打个数。一般I,k,m均可认为是常量，所以时间和空间复杂度可以简化为O(n)，即线性的。


/************************
		k-means++算法
*************************/
算法基本思想：对kmeans算法中的初始聚类中心选择原则做改进，初始聚类中心之间的距离尽可能大。

算法步骤：
1、从样本数据集中随机选择一个样本作为质心；
2、遍历样本数据集，计算每个样本与其最近质心的距离D(i)；
3、选择一个新的样本作为质心，选择的规则是：D(i)较大的样本，被选取为质心的概率较大；
4、重复步骤2和3，直到k个质心被选出来；
5、利用这k个初始质心运行标准k-means算法，

步骤3的规则的一种实现方法如下：
1、计算所有D(i)的总和为Sum；
2、在0-Sum的范围内抽取一个随机数R；
3、遍历所有D(i)，直至R-=D(i)<=0为止，选取第i个样本作为新的质心；

算法设计：
	算法输入：样本数据、簇数
	算法输出：各样本所属的簇，以及该样本到最近质心的距离。
	样本设计：样本属性包含样本数据、样本维数以、样本所属簇、以及该样本到最近质心的距离。
	过程设计：包含质心初始化、距离计算和样本归类、新质心计算和新旧质心比照、条件循环
	过程变量设计：存储质心数组变量、存储各个样本与质心的距离数组变量。

对Sample结构增加数据成员min_dis，表示该样本到最近质心的距离
typedef struct Sample{
int cluster;
vector<float> data;
float min_dis;
};

/************************
		轮廓系数
*************************/
对于样本集中的样本i：
计算i到簇内个样本的相异度的平均值，即i样本的内聚度，记ai；
计算i到其他簇的相异度的平均值的最小值，即i样本的分离度，记bi；
定义样本i的轮廓系数：
si = (bi - ai) / max{ai,bi}
定义样本集的总体轮廓系数为
s = avg(si)
显然si和s落在(-1,1)区间，并且s越接近1，表示算法聚类效果越好。



/************************
		数据预处理方法
*************************/
0-1规格化




1、读取iris数据集做测试、数据可视化
2、测试k = 2、3、4、、5、6、7、8的轮廓系数
3、测试分段随机、kmeans++、长度等分等轮廓系数
4、测试欧式距离、闵可夫斯基距离、曼哈顿距离的轮廓系数

